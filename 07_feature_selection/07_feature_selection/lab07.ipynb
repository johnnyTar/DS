{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Lab 5: Feature Engineering\n",
    "\n",
    "Let's get started with the initialization of the notebook by importing the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import load_dataset\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Uber Movement Speeds Dataset For Berlin\n",
    "\n",
    "To enable easy visualization of the model fitting process we will use a simple traffic speeds dataset, provided by Uber at https://movement.uber.com/cities/berlin/downloads/speeds?lang=en-US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/movement-speeds-hourly-berlin-2020-3-joint-location.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute an absolute time reference, using the day and hour. Times will start at 0--representing March 1, 2020 12-1 AM--incrementing by 1 at a time until 192--representing March 8, 2020 11 PM - 12 AM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['time'] = ...\n",
    "df = df.sort_values(by='time')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the average movement speed over time, aggregating across all locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "time = df.groupby(by=['time']).agg('mean').reset_index()\n",
    "plt.plot(time['time'], time['speed_kph_mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Linear Models with Scikit-Learn\n",
    "\n",
    "Notebook by Joseph E. Gonzalez, Alvin Wan\n",
    "\n",
    "In this lesson, we introduce the normal equations as well as several other algorithms to provide some insight behind how these techniques work and perhaps more importantly how they fail.  However, in practice you will seldom need to implement the core algorithms and will instead use various machine learning software packages.  In this class, we will focus on the widely used scikit-learn package.\n",
    "\n",
    "Scikit-learn, or as the cool kids call it sklearn (pronounced s-k-learn), is an large package of useful machine learning algorithms. For this lecture, we will use the `LinearRegression` model in the [`linear_model`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model) module.  The fact that there is an entire module with many different models within the `linear_model` module might suggest that we have a lot to cover still (we do!).\n",
    "\n",
    "**What you should know about `sklearn` models:**\n",
    "\n",
    "1. Models are created by first building an instance of the model:\n",
    "```python\n",
    "model = SuperCoolModelType(args)\n",
    "```\n",
    "1. You then fit the model by calling the **fit** function passing in data:\n",
    "```python\n",
    "model.fit(X, Y)\n",
    "```\n",
    "1. You then can make predictions by calling **predict**:\n",
    "```python\n",
    "model.predict(X)\n",
    "```\n",
    "\n",
    "The neat part about sklearn is most models behave like this.  So if you want to try a cool new model you just change the class of model you are using.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fit OLS Model using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_y_vs_yhat(df, y, yhat):\n",
    "    plt.figure()\n",
    "    Y, Yhat = df[y], df[yhat]\n",
    "    plt.scatter(Yhat, Y, label='(yhat, y)')\n",
    "    cmin, cmax = max(Yhat.min(), Y.min()), min(Yhat.max(), Y.max())\n",
    "    plt.plot([cmin, cmax], [cmin, cmax], color='red', label='y=yhat')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(df, x, y, yhat):\n",
    "    plt.figure()\n",
    "    X, Y, Yhat = df[x], df[y], df[yhat]\n",
    "    plt.plot(X, Y, label='ground truth')\n",
    "    plt.plot(X, Yhat, label='prediction')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_over_time(df, x, y, yhat):\n",
    "    time = df.groupby(by='time').agg('mean').reset_index()\n",
    "    plot_predictions(time, x, y, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the `LinearRegression` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of the model. Like before, we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.a Train OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model by passing it the $X$ and $Y$ data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X, Y = df[['time']], df[[\"speed_kph_mean\"]] # extract data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.b Predict with OLS\n",
    "\n",
    "Make some predictions and even save them back to the original DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['Yhat_sklearn'] = Yhat = model.predict(X)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.c Analyze Fit with OLS\n",
    "\n",
    "Analyzing the fit again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_y_vs_yhat(df.sample(frac=0.01), y=\"speed_kph_mean\", yhat=\"Yhat_sklearn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the residual distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['residuals_sklearn'] = df['speed_kph_mean'] - df['Yhat_sklearn']\n",
    "_ = plt.hist(df.sample(frac=0.01)['residuals_sklearn'], bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.d Evaluate OLS using Scikit-Learn\n",
    "\n",
    "As we tune the features in our model it will be important to define some useful error metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Mean Squared Error:\", mean_squared_error(Y, Yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Mean Absolute Error:\", mean_absolute_error(Y, Yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Root Mean Squared Error:\", np.sqrt(mean_squared_error(Y, Yhat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Standard Deviation of Residuals:\", np.std(df['residuals_sklearn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we play with the model we might want a standard visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(df, y, yhat):\n",
    "    \"\"\"Compute and print error metrics\"\"\"\n",
    "    Y, Yhat = df[y], df[yhat]\n",
    "    metrics = {\n",
    "        'MSE': mean_squared_error(Y, Yhat),\n",
    "        'MAE': mean_absolute_error(Y, Yhat),\n",
    "        'RMSE': np.sqrt(mean_squared_error(Y, Yhat)),\n",
    "    }\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_and_plot(df, x, y, yhat):\n",
    "    \"\"\"Report error metrics and also visualize\"\"\"\n",
    "    evaluate(df, y, yhat)\n",
    "    plot_y_vs_yhat(df.sample(frac=0.01), y, yhat)\n",
    "    plot_predictions_over_time(df, x, y, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining our latest model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_and_plot(df, x='time', y='speed_kph_mean', yhat='Yhat_sklearn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Fit Biased OLS using Scikit-Learn\n",
    "\n",
    "Redo the above except using a model with the intercept term. This is as simple as simply passing `fit_intercept=True` to the `LinearRegression` model constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "biased = ... # create model\n",
    "... # train model\n",
    "df['Yhat'] = ... # predict with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_and_plot(df, x='time', y='speed_kph_mean', yhat='Yhat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's amend our table of results with the additional metrics above.\n",
    "\n",
    "||MSE|MAE|RMSE|\n",
    "|---|---|---|---|\n",
    "|**OLS**|643|19.8|25.3|\n",
    "|**Biased OLS**|213|10.6|14.6|\n",
    "\n",
    "Examining the above data we see that there is some **periodic** structure as well as some **curvature**. Can we fit this data with a linear model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that during the lecture we learned that feature engineering help us to achieve 3 main goals:\n",
    "\n",
    "1. Express non-linear relationships.\n",
    "\n",
    "2. Capture domain knowledge.\n",
    "\n",
    "3. Encode non-numeric features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Non-linear Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook by Joseph E. Gonzalez, Alvin Wan\n",
    "\n",
    "In this notebook, we will use basic feature transformations (feature engineering) to model non-linear relationships using linear models.\n",
    "\n",
    "**What does it mean to be a _Linear Model_?**\n",
    "\n",
    "Linear models are **linear combinations** of features.  These models are therefore linear in the **parameters** but not necessarily the underlying data.  We can encode non-linearity in our data through the use of feature functions:\n",
    "\n",
    "\n",
    "$$\n",
    "f_\\theta\\left( x \\right) = \\phi(x)^T \\theta = \\sum_{j=0}^{p} \\phi(x)_j \\theta_j\n",
    "$$\n",
    "\n",
    "where $\\phi$ is an *arbitrary function* from $x\\in \\mathbb{R}^d$ to $\\phi(x) \\in \\mathbb{R}^{p+1}$. We could also denote these as a collection of separate feature $\\phi_j$ feature functions from $x\\in \\mathbb{R}^d$ to $\\phi_j(x) \\in \\mathbb{R}$:\n",
    "\n",
    "$$\n",
    "\\phi(x) = \\left[\\phi_0(x), \\phi_1(x), \\ldots, \\phi_p(x) \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "We often refer to these $\\phi_j$ as **feature functions** and their design plays a critical role in both how we capture prior knowledge and our ability to fit complicated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fit Biased OLS Model\n",
    "\n",
    "We'll expand the data features we're allowed to use. Instead of just taking in time, our OLS model will now take in time and location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X, Y = df[['Latitude', 'Longitude', 'time']], df[[\"speed_kph_mean\"]] # extract data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(X, Y)\n",
    "df['Yhat'] = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_and_plot(df, x='time', y='speed_kph_mean', yhat='Yhat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see our results so far.\n",
    "\n",
    "\n",
    "||MSE|MAE|RMSE|\n",
    "|---|---|---|---|\n",
    "|**OLS**|643|19.8|25.3|\n",
    "|**Biased OLS**|213|10.6|14.6|\n",
    "|**Biased OLS + Location**|201|10.7|14.2|\n",
    "\n",
    "Examining the above data we see that there is some **periodic** structure as well as some **curvature**. Can we fit this data with a linear model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Polynomial Features\n",
    "\n",
    "There is some curvature.  We can introduce polynomial terms to try to improve the fit of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def phi_curved(X):\n",
    "    return np.hstack([\n",
    "        X,\n",
    "        X * X,\n",
    "        np.expand_dims(np.prod(X, axis=1), 1),\n",
    "        X ** 3,\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you guess the new number of features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "curvedX = phi_curved(X)\n",
    "curvedX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "curved = LinearRegression()\n",
    "curved.fit(curvedX, Y)\n",
    "df['Yhat_curved'] = curved.predict(curvedX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_and_plot(df, x='time', y='speed_kph_mean', yhat='Yhat_curved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our results so far, we see that higher-order polynomial terms actually improved our best error by 16%.\n",
    "\n",
    "||MSE|MAE|RMSE|\n",
    "|---|---|---|---|\n",
    "|**OLS**|643|19.8|25.3|\n",
    "|**Biased OLS**|213|10.6|14.6|\n",
    "|**Biased OLS + Location**|201|10.7|14.2|\n",
    "|**Biased OLS + Location + Poly**|175|10.1|13.2|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sinusoidal Features\n",
    "\n",
    "In the following, we will add a few different sine functions at different frequencies and offsets.\n",
    "\n",
    "$$\n",
    "\\sin\\left(2 \\pi * \\textbf{frequency}X + \\textbf{phase}\\right)\n",
    "$$\n",
    "\n",
    "Note that for this to remain a linear model, we cannot make the frequency or phase of the sine function a model parameter.  In fact, these are actually **hyperparameters** of the model that would need to be tuned using either domain knowledge or other search procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def phi_periodic(X):\n",
    "    return np.hstack([\n",
    "        X,\n",
    "        np.sin(X),\n",
    "        np.sin(0.26*X),\n",
    "        np.sin(X - 6),\n",
    "        np.sin(0.26 * X - 6),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "phi_periodic(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine all the features we have so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def phi_curved_and_periodic(X):\n",
    "    return np.hstack([phi_curved(X), phi_periodic(X)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "crazyX = ...\n",
    "crazyX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that to make predictions I need to actually apply the $\\Phi$ feature function to my data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "crazy = LinearRegression()\n",
    "crazy.fit(crazyX, Y)\n",
    "df['Yhat_crazy'] = crazy.predict(crazyX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_and_plot(df, x='time', y='speed_kph_mean', yhat='Yhat_crazy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our final table of results, our sinusoidal features improved our best error by 6%. Compared with our original OLS result, we've improved our error by 62%, reducing from 346 MSE by over 2x to 132 MSE.\n",
    "\n",
    "||MSE|MAE|RMSE|\n",
    "|---|---|---|---|\n",
    "|**OLS**|643|19.8|25.3|\n",
    "|**Biased OLS**|213|10.6|14.6|\n",
    "|**Biased OLS + Location**|201|10.7|14.2|\n",
    "|**Biased OLS + Location + Poly**|175|10.1|13.2|\n",
    "|**Biased OLS + Location + Poly + Sin**|167|9.9|12.9|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success!\n",
    "\n",
    "Using non-linear feature functions, we're now able to model non-linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Imputing Missing Values with Scikit-Learn\n",
    "\n",
    "\n",
    "In this notebook, we discuss how to deal with missing values. In the process, we will work through feature engineering to construct a model that predicts vehicle efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Load `mpg` Dataset\n",
    "\n",
    "For this notebook, we will use the seaborn `mpg` data set which describes the fuel mileage (measured in miles per gallon or mpg) of various cars along with characteristics of those cars.  Our goal will be to build a model that can predict the fuel mileage of a car based on the characteristics of that car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/mpg.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice a large number of columns are not quantitative continuous. Ignore these for now. We will deal with in next lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Y = data[[\"mpg\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Keeping Track of Progress\n",
    "\n",
    "Because we are going to be building multiple models with different feature functions it is important to have a standard way to track each of the models.\n",
    "\n",
    "The following function takes a model prediction function, the name of a model, and the dictionary of models that we have already constructed.  It then evaluates the new model on the data and plots how the new model performs relative to the previous models as well as the $Y$ vs $\\hat{Y}$ scatter plot.\n",
    "\n",
    "In addition, it updates the dictionary of models to include the new model for future plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_and_plot_mpg(name, df, y, yhat):\n",
    "    metrics = evaluate(df, y, yhat)\n",
    "    plot_y_vs_yhat(df, y, yhat)\n",
    "\n",
    "    results[name] = metrics\n",
    "    return pd.DataFrame(results).sort_values(by='MSE', axis=1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Imputing Missing Quantitative Continuous Features\n",
    "\n",
    "This data set has several quantitative continuous features that we can use to build our first model.  However, even for quantitative continuous features, we may want to do some additional feature engineering.  Things to consider are:\n",
    "\n",
    "1. transforming features with non-linear functions (log, exp, sine, polynomials)\n",
    "2. constructing products or ratios of features\n",
    "3. dealing with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "\n",
    "We can use the Pandas `DataFrame.isna` function to find rows with missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# show the rows that contain a NaN value\n",
    "data[data.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to deal with missing values.  A common strategy is to substitute the mean.  Because missing values can actually be useful signal, it is often a good idea to include a feature indicating that the value was missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def impute_mpg(df):\n",
    "    Phi = df[[\"cylinders\", \"displacement\",\n",
    "              \"horsepower\", \"weight\",\n",
    "              \"acceleration\",\n",
    "              \"model_year\"]].copy()\n",
    "    Phi[\"horsepower_missing\"] = Phi[\"horsepower\"].isna()\n",
    "    Phi = Phi.fillna(Phi.mean())\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Biased OLS Model\n",
    "\n",
    "Using our feature function, we can fit our first model to the transformed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_model_with_phi(df, phi, X, Y):\n",
    "    model = LinearRegression()\n",
    "    Phi = phi(X)\n",
    "    model.fit(Phi, Y)\n",
    "    yhat = model.predict(Phi)\n",
    "    return model, yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "basic, yhat = train_model_with_phi(...\n",
    "data['Yhat'] = yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_and_plot_mpg('basic', data, y=\"mpg\", yhat=\"Yhat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Stable Feature Functions\n",
    "\n",
    "Unfortunately, the feature function we just implemented applies a different transformation depending on what input we provide. Specifically, if the `horsepower` is missing when we go to make a prediction we will substitute it with a different mean then was used when we fit our model.  Furthermore, if we only want predictions on a few records and the `horsepower` is missing from those records then the feature function will be unable to substitute a meaningful value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if we were to get new records that look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_data = data[data['horsepower'].isna()].head(3)\n",
    "new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature function is be unable to substitute the mean since none of the records have a `horsepower` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    basic.predict(impute_mpg(new_data))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fix this by computing the mean on the original data and using that mean on any new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Making a global variable\n",
    "def impute_mpg(df, data_mean = data.mean()):\n",
    "    feature_cols = [\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model_year\"]\n",
    "    Phi = df[feature_cols].copy()\n",
    "    Phi[\"horsepower_missing\"] = Phi[\"horsepower\"].isna().astype(float)\n",
    "    Phi = Phi.fillna(data_mean)\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_mpg(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "basic.predict(impute_mpg(new_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Scikit-learn Model Imputer\n",
    "\n",
    "Because these kinds of transformations are fairly common. Scikit-learn has built-in transformations for data imputation.  These transformations have a common pattern of `fit` and `transform`.  You first `fit` the transformation to your data and then you can `transform` your data and any future data using the same transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "imputer.fit(data[['weight', 'horsepower']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "imputer.transform(data[['weight', 'horsepower']])[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "imputer.fit(data[['horsepower']])\n",
    "def impute_mpg_sklearn(df, imputer=imputer):\n",
    "    feature_cols = [\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model_year\"]\n",
    "    Phi = df[feature_cols].copy()\n",
    "    Phi[\"horsepower_missing\"] = Phi[\"horsepower\"].isna().astype(float)\n",
    "    Phi[\"horsepower\"] = imputer.transform(Phi[[\"horsepower\"]]).flatten()\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "basicsk, data['Yhat'] = train_model_with_phi(data, impute_mpg_sklearn, data, Y)\n",
    "evaluate_and_plot_mpg(\"basic_sklearn\", data, y=\"mpg\", yhat=\"Yhat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Applying Domain Knowledge\n",
    "\n",
    "Let's try improving the model by applying feature functions from before: polynomial and sinusoidal features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The displacement of an engine is defined as the product of the volume of each cylinder and number of cylinders.  However, not all cylinders fire at the same time (at least in a functioning engine) so the fuel economy might be more closely related to the volume of any one cylinder.\n",
    "\n",
    "\n",
    "## 7.1 Displacement Features\n",
    "\n",
    "We can use this \"domain knowledge\" to compute a new feature encoding the volume per cylinder by taking the ratio of displacement and cylinders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def phi_with_displacement(df):\n",
    "    Phi = impute_mpg_sklearn(df)\n",
    "    Phi['displacement/cylinder'] = ...\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again fitting and evaluating our model we see a reduction in prediction error (RMSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "disp, data['Yhat_disp'] = train_model_with_phi(data, phi_with_displacement, data, Y)\n",
    "evaluate_and_plot_mpg(\"disp\", data, y=\"mpg\", yhat=\"Yhat_disp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Polynomial Features\n",
    "\n",
    "Let's apply the feature functions we explored in the previous lesson. Do they work here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def phi_crazy(df):\n",
    "    Phi = impute_mpg_sklearn(df)\n",
    "    Phi = phi_curved(Phi)\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "disp, data['Yhat_crazy'] = train_model_with_phi(data, phi_crazy, data, Y)\n",
    "evaluate_and_plot_mpg(\"crazy\", data, y=\"mpg\", yhat=\"Yhat_crazy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Sinusoidal Features\n",
    "\n",
    "Those seemed to work well. Let's try more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def phi_crazier(df):\n",
    "    Phi = impute_mpg_sklearn(df)\n",
    "    Phi = phi_curved_and_periodic(Phi)\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "disp, data['Yhat_crazier'] = train_model_with_phi(data, phi_crazier, data, Y)\n",
    "evaluate_and_plot_mpg(\"crazier\", data, y=\"mpg\", yhat=\"Yhat_crazier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This random hodge podge of features is what we call \"feature soup\". It's senseless feature mashing to get a better result. We'll see why specifically this is bad, in future lectures. For now, it looks like feature soup is getting diminishing returns and has plateau'ed in performance. Hitting a wall here, we'll now turn to an alternative: Below, we'll leverage insights about the problem, our domain knowledge, to *further* significantly improve our model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Encoding Non-Numeric and Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Encoding Categorical Data\n",
    "\n",
    "The `origin` column in this data set is categorical (nominal) data taking on a fixed set of possible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_ = plt.hist(data['origin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this kind of data in a model, we need to transform into a vector encoding that treats each distinct value as a separate dimension.  This is called One-hot Encoding or Dummy Encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.1 One-Hot Encoding (Dummy Encoding)\n",
    "\n",
    "\n",
    "One-Hot encoding, sometimes also called **dummy encoding** is a simple mechanism to encode categorical data as real numbers such that the magnitude of each dimension is meaningful.  Suppose a feature can take on $k$ distinct values (e.g., $k=50$ for 50 states in the United Stated).  A new feature (dimension) is created for each distinct value.  For each record, all the new features are set to zero except the one corresponding to the value in the original feature.\n",
    "\n",
    "<img src=\"images/one_hot_state.png\" width=\"600px\">\n",
    "\n",
    "The term one-hot encoding comes from a digital circuit encoding of a categorical state as particular \"hot\" wire:\n",
    "\n",
    "<img src=\"images/one_hot_encoding.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.2  Dummy Encoding in Pandas\n",
    "\n",
    "We can construct a one-hot (dummy) encoding of the origin column using the `Pandas.get_dummies` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(data[['origin']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `Pandas.get_dummies`, we can build a new feature function which extends our previous features with the additional dummy encoding columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def phi_with_origin(df):\n",
    "    Phi = phi_with_displacement(df)\n",
    "    Phi = Phi.join(pd.get_dummies(df[['origin']]))\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit a new model with the origin feature encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "oh, data['Yhat_oh'] = train_model_with_phi(data, phi_with_origin, data, Y)\n",
    "evaluate_and_plot_mpg(\"oh\", data, y=\"mpg\", yhat=\"Yhat_oh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the above feature function is not stable.  For example, if we are given a single vehicle to make a prediction the model will fail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    oh.predict(phi_with_origin(data.head(1)))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see why this fails look at the feature transformation for a single row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "phi_with_origin(data.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dummy columns are not created for the other categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple solutions.  We could maintain a list of dummy columns and always add these columns.  Alternatively, we could use a library function designed to solve this problem.  The second option is much easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.3 Scikit-learn One-hot Encoder\n",
    "\n",
    "The scikit-learn library has a wide range feature transformations and a framework for composing them in reusable (stable) pipelines.  Let's first look at a basic [`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "oh_enc = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then fit that instance to some data.  This is where we would determine the specific values that a categorical feature can take:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "oh_enc.fit(data[['origin']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we fit the transformation, we can then use it transform new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "oh_enc.transform(data[['origin']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "oh_enc.transform(data[['origin']].head()).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect the categories of the one-hot encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "oh_enc.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can update our feature function to use the one-hot encoder instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def phi_with_origin(df):\n",
    "    Phi = phi_with_displacement(df)\n",
    "    dummies = pd.DataFrame(oh_enc.transform(df[['origin']]).todense(),\n",
    "                           columns=oh_enc.get_feature_names(),\n",
    "                           index = df.index)\n",
    "    return Phi.join(dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "phi_with_origin(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model = LinearRegression()\n",
    "# model.fit(phi_with_origin(data), data[[\"mpg\"]])\n",
    "# evaluate_model(\"cont.+(d/c)+o\", model, phi_with_origin, models)\n",
    "oh, data['Yhat_oh'] = train_model_with_phi(data, phi_with_origin, data, Y)\n",
    "evaluate_and_plot_mpg(\"oh_sklearn\", data, y=\"mpg\", yhat=\"Yhat_oh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Encoding Text using Bag-of-Words\n",
    "\n",
    "The only remaining feature to encode is the vehicle name.  Is there potentially signal in the vehicle name?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data[['name']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding text can be challenging.  The capturing the semantics and grammar of language in mathematical (vector) representations is an active area of research.  State-of-the-art techniques often rely on neural networks trained on large collections of text. In this class, we will focus on basic text encoding techniques that are still widely used.  If you are interested in learning more, checkout [BERT Explained: A Complete Guide with Theory and Tutorial](https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here we present two widely used representations of text:\n",
    "\n",
    "* **Bag-of-Words Encoding**: encodes text by the frequency of each word\n",
    "* **N-Gram Encoding**: encodes text by the frequency of sequences of words of length $N$\n",
    "\n",
    "Both of these encoding strategies are related to the one-hot encoding with dummy features created for every word or sequence of words and with multiple dummy features having counts greater than zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.1 The Bag-of-Words Encoding\n",
    "\n",
    "\n",
    "The bag-of-words encoding is widely used and a standard representation for text in many of the popular text clustering algorithms.  The following is a simple illustration of the bag-of-words encoding:\n",
    "\n",
    "<img src=\"images/bag_of_words.png\" width=\"600px\">\n",
    "\n",
    "**Notice**\n",
    "1. **Stop words are often removed.** Stop-words are words like `is` and `about` that in isolation contain very little information about the meaning of the sentence.  Here is a good list of [stop-words in many languages](https://code.google.com/archive/p/stop-words/).\n",
    "1. **Word order information is lost.**  Nonetheless the vector still suggests that the sentence is about `fun`, `machines`, and `learning`.  Thought there are many possible meanings _learning machines have fun learning_ or _learning about machines is fun learning_ ...\n",
    "1. **Capitalization and punctuation are typically removed.**  However, emoji symbols may be worth preserving.\n",
    "1. **Sparse Encoding:** is necessary to represent the bag-of-words efficiently.  There are millions of possible words (including terminology, names, and misspellings) and so instantiating a `0` for every word that is not in each record would be inefficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2 Bag-of-words in Scikit-learn\n",
    "\n",
    "We can use scikit-learn to construct a bag-of-words representation of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "frost_text = [x for x in \"\"\"\n",
    "Some say the world will end in fire,\n",
    "Some say in ice.\n",
    "From what Ive tasted of desire\n",
    "I hold with those who favor fire.\n",
    "\"\"\".split(\"\\n\") if len(x) > 0]\n",
    "\n",
    "frost_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Construct the tokenizer with English stop words\n",
    "bow = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# fit the model to the passage\n",
    "bow.fit(frost_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Print the words that are kept\n",
    "print(\"Words:\", list(enumerate(bow.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Sentence Encoding: \\n\")\n",
    "# Print the encoding of each line\n",
    "for (text, encoding) in zip(frost_text, bow.transform(frost_text)):\n",
    "    print(text)\n",
    "    print(encoding.todense())\n",
    "    print(\"------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Encoding Text using N-Gram Encoding\n",
    "\n",
    "The n-gram encoding is a generalization of the bag-of-words encoding designed to capture information about word ordering.  Consider the following passage of text:\n",
    "\n",
    "> _The book was not well written but I did enjoy it._\n",
    "\n",
    "If we re-arrange the words we can also write:\n",
    "\n",
    "> _The book was well written but I did not enjoy it._\n",
    "\n",
    "Moreover, local word order can be important when making decisions about text.  The n-gram encoding captures local word order by defining counts over sliding windows. In the following example a bi-gram ($n=2$) encoding is constructed:\n",
    "\n",
    "<img src=\"images/ngram.png\" width=\"800px\">\n",
    "\n",
    "The above n-gram would be encoded in the sparse vector:\n",
    "\n",
    "<img src=\"images/ngram_vector.png\" width=\"300px\">\n",
    "\n",
    "Notice that the n-gram captures key pieces of sentiment information: `\"well written\"` and `\"not enjoy\"`.\n",
    "\n",
    "N-grams are often used for other types of sequence data beyond text. For example, n-grams can be used to encode genomic data, protein sequences, and click logs.\n",
    "\n",
    "**N-Gram Issues**\n",
    "1. Maintaining the dictionary of possible n-grams can be very costly.  There are several approximations leveraging hashing that can be used to closely approximate n-gram encoding without the need to maintain the dictionary of all possible n-grams.\n",
    "1. As the size $n$ of n-grams increases the chance of observing more than one instance decreases limiting their value as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Construct the tokenizer with English stop words\n",
    "bigram = CountVectorizer(ngram_range=(1, 2))\n",
    "# fit the model to the passage\n",
    "bigram.fit(frost_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Print the words that are kept\n",
    "print(\"\\nWords:\",\n",
    "      list(zip(range(0,len(bigram.get_feature_names())), bigram.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nSentence Encoding: \\n\")\n",
    "# Print the encoding of each line\n",
    "for (text, encoding) in zip(frost_text, bigram.transform(frost_text)):\n",
    "    print(text)\n",
    "    print(encoding.todense())\n",
    "    print(\"------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.1 Applying Text Encoding\n",
    "\n",
    "We can add the text encoding features to our feature function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bow = CountVectorizer()\n",
    "bow.fit(data[\"name\"])\n",
    "\n",
    "def phi_with_name(df):\n",
    "    Phi = phi_with_origin(df)\n",
    "    bow_encoding = pd.DataFrame(\n",
    "        bow.transform(df['name']).todense(),\n",
    "        columns=bow.get_feature_names(),\n",
    "        index = df.index)\n",
    "    Phi = Phi.join(bow_encoding)\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Phi = phi_with_name(data)\n",
    "Phi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model = LinearRegression()\n",
    "# model.fit(phi_with_name(data), data[[\"mpg\"]])\n",
    "# evaluate_model(\"cont.+(d/c)+o+n\", model, phi_with_name, models)\n",
    "\n",
    "name, data['Yhat_name'] = train_model_with_phi(data, phi_with_name, data, Y)\n",
    "evaluate_and_plot_mpg(\"name\", data, y=\"mpg\", yhat=\"Yhat_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Success!!!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
